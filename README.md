# **ğŸš€ Fine-Tuned GPT-Neo 2.7B for Question Answering**  

## **ğŸ“Œ Project Overview**  
This project is focused on **fine-tuning GPT-Neo 2.7B** for **question-answering tasks** using the **SQuAD v2 dataset**. The goal was to optimize the model for **faster training, lower memory usage, and improved performance** using **DeepSpeed, LoRA, and mixed precision (bf16) training**.  

The fine-tuned model can be used in various NLP applications, including:  
- **Conversational AI** ğŸ¤–  
- **Automated Q&A Systems** ğŸ“š  
- **Information Retrieval** ğŸ”  

Since working with large-scale LLMs can be resource-intensive, this project focuses on **efficient fine-tuning techniques** to get the best results without requiring high-end hardware.  

---

## **ğŸ“‹ What You Need to Reproduce This Project**  

### **ğŸ”¹ System Requirements**  
- A **GPU-enabled setup** (Google Colab, AWS, or a local machine with CUDA).  
- At least **16GB VRAM recommended** for fine-tuning.  

### **ğŸ”¹ Libraries & Frameworks**  
To fine-tune or run the model, youâ€™ll need:  
- **Hugging Face Transformers** â€“ for working with GPT-Neo.  
- **DeepSpeed** â€“ to speed up training and reduce memory usage.  
- **PyTorch** â€“ for training and inference.  
- **LoRA (Low-Rank Adaptation)** â€“ for more efficient fine-tuning.  
- **Accelerate** â€“ for optimized multi-GPU training.  
- **Weights & Biases (Optional)** â€“ for experiment tracking.  

### **ğŸ”¹ Dataset Used**  
- **SQuAD v2** â€“ a benchmark dataset for Question Answering.  
- Since training on the full dataset is resource-intensive, **only 2%** was used for ultra-fast fine-tuning.  

### **ğŸ”¹ Model Weights & Storage**  
- The full **fine-tuned model weights** are **too large for GitHub**, so they are stored externally.  
- **Download `pytorch_model.bin` from:**  
  ğŸ”— [Google Drive Link](https://colab.research.google.com/drive/1vVzHQYCJvobUpegnJ0Jowp_ZBGsJWMzR?usp=sharing)  

---

## **ğŸ“¦ Project Deliverables**  
This repository contains everything needed to fine-tune GPT-Neo 2.7B and reproduce the results.  

### **ğŸ“‚ Files in This Repo**  
- **`preprocess.py`** â€“ Tokenizes and prepares the dataset.  
- **`train.py`** â€“ Fine-tunes GPT-Neo 2.7B using DeepSpeed & LoRA.  
- **`ds_config.json`** â€“ Configuration file for DeepSpeed optimization.  
- **`README.md`** â€“ Documentation for setup and model access.  

### **ğŸ“‚ Model Files (Stored Externally)**  
- **Model Configurations** (`config.json`, `tokenizer.json`, etc.).  
- **LoRA Adapter Weights** (`adapter_model.safetensors`).  
- **Tokenizer & Vocabulary** (`vocab.json`, `merges.txt`).  
- **Training Logs & Checkpoints**.  
---

## **ğŸš€ How the Model Was Trained**  

### **1ï¸âƒ£ Preprocessing the Dataset**  
- The **SQuAD v2 dataset** was tokenized with a **max sequence length of 128** for faster training.  
- **Padding & truncation** were applied to ensure consistent input sizes.  

### **2ï¸âƒ£ Fine-Tuning Process**  
- Used **DeepSpeed Zero-Offload (Stage 2)** to handle large model sizes efficiently.  
- **LoRA adapters** were applied to reduce the number of trainable parameters.  
- **bf16 Mixed Precision** was enabled to speed up training and reduce memory usage.  

### **3ï¸âƒ£ Model Evaluation**  
- The fine-tuned model was tested on **2% of the SQuAD v2 validation set**.  
- Performance was measured using **accuracy, loss, and perplexity scores**.  

### **4ï¸âƒ£ Deployment Possibilities (Optional)**  
- The model can be **converted to ONNX** for lightweight deployment.  
- It can be **hosted using FastAPI** to serve real-time responses.  

---

## **ğŸ“Š Model Performance & Results**  

âœ… Successfully fine-tuned **GPT-Neo 2.7B** on a subset of **SQuAD v2**  
âœ… Integrated **DeepSpeed Zero-Offload** for efficient memory usage  
âœ… Used **LoRA adapters** to fine-tune without retraining the full model  
âœ… Enabled **mixed precision (bf16)** to speed up training  
âœ… Achieved significant improvements in **Question Answering tasks**  

---

## **ğŸ“¥ Download & Use the Fine-Tuned Model**  

Since GitHub has file size limitations, **the full model weights must be downloaded separately**:  

ğŸ“¥ **Download `pytorch_model.bin` from:**  
- **Google Drive:** [Download Link](https://colab.research.google.com/drive/1vVzHQYCJvobUpegnJ0Jowp_ZBGsJWMzR?usp=sharing)  

---

## **ğŸ† Future Plans for the Project**  
ğŸ”¹ Experimenting with **larger datasets for improved performance**  
ğŸ”¹ **Tuning hyperparameters** to optimize accuracy  
ğŸ”¹ **Deploying the model as a real-time API** for live Question Answering  
ğŸ”¹ **Comparing GPT-Neoâ€™s performance with other LLMs** like GPT-J, BLOOM, Falcon  

---

## **ğŸ‘¨â€ğŸ’» Project Contributors**  

- **[Bhimavarapu Saiteja Reddy](https://www.linkedin.com/in/Teja220)** â€“ Model fine-tuning, optimization, and training.  

ğŸ“§ **For any questions or collaborations, feel free to reach out:**  
- **LinkedIn:** https://www.linkedin.com/in/Teja220  
- **Email:** saitejaredyy@gmail.com  
